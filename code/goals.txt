Title:
DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability
URL:
https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_DiffDis_Empowering_Generative_Diffusion_Model_with_Cross-Modal_Discrimination_Capability_ICCV_2023_paper.pdf

Quantiative Experiment Goal:
Table 1:
Row4: DiffDis on CIFAR10 and CIFAR100 datasets

Qualitative Experiment Goal:
Figure 4. Qualitative comparisons of Stable diffusion and DiffDis on MSCOCO zero-shot text-to-image generation.

—— version 1 submission ——

1. We did not change our experimental goals
2. To validate our model's training, we're using a dummy dataset that mimics the original CC3M dataset. Due to the CC3M's large size (~430 GB) and our resource constraints, completing training before the May 5 deadline for the first version is unfeasible. However, we aim to finish training by the second version's deadline on May 31.
3. We've discovered that the transformer for the image-to-text task is a separate entity from our UNet model. It consists of self-attention blocks, indicating it could be constructed using UNet's self-attention blocks for a simpler implementation. However, this approach is still being considered.

—— version 2 submission ——

1. We did not change our experimental goals.
2. We reproduced the results we have targeted. For quantiative experiments, we used the CIFAR10 and CIFAR100 datasets and achieved the 91.27% and 90.26% accuracy respectively. For qualitative experiments, we used the MSCOCO dataset. The generated images were not as good as the original paper's results, but they were still acceptable.
3. You can look at the challenges we faced in the main jupyter notebook.