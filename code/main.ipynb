{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paper Name:** DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability\n",
    "\n",
    "**Link:** https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_DiffDis_Empowering_Generative_Diffusion_Model_with_Cross-Modal_Discrimination_Capability_ICCV_2023_paper.pd\n",
    "\n",
    "**Project Members:**\n",
    "Furkan Genç,\n",
    "Barış Sarper Tezcan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the constants \n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "LATENTS_WIDTH = WIDTH // 8\n",
    "LATENTS_HEIGHT = HEIGHT // 8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# training parameters\n",
    "first_epoch = 0\n",
    "num_train_epochs = 10\n",
    "latents_shape = (1, 4, LATENTS_HEIGHT, LATENTS_WIDTH)\n",
    "Lambda = 1.0\n",
    "\n",
    "# optimizer parameters\n",
    "learning_rate = 1e-4\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_weight_decay = 0.0\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# checkpoint parameters\n",
    "checkpoints_total_limit = 1\n",
    "output_dir = \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from ddpm import DDPMSampler\n",
    "from pipeline import get_time_embedding\n",
    "from dataloader import train_dataloader\n",
    "import model_loader\n",
    "import time\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "model_file = \"./data/v1-5-pruned.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "vae = models['encoder']\n",
    "text_encoder = models['clip']\n",
    "decoder = models['decoder']\n",
    "unet = models['diffusion']\n",
    "ddpm = DDPMSampler(generator=None)\n",
    "\n",
    "# Disable gradient computations for the VAE, DDPM, and text_encoder models\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# set the vae and text_encoder to eval mode\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate, betas=(adam_beta1, adam_beta2), weight_decay=adam_weight_decay, eps=adam_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_train_epochs, device=\"cuda\", save_steps=1000, max_train_steps=10000):\n",
    "    global_step = 0\n",
    "\n",
    "    # create the output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # move models to the device\n",
    "    vae.to(device)\n",
    "    text_encoder.to(device)\n",
    "    unet.to(device)\n",
    "\n",
    "    num_train_epochs = tqdm(range(first_epoch, num_train_epochs), desc=\"Epoch\")\n",
    "    for epoch in num_train_epochs:\n",
    "        train_loss = 0.0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # batch consists of images and texts, we need to extract the images and texts\n",
    "\n",
    "            # move batch to the device\n",
    "            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "            batch[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
    "\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            encoder_noise = torch.randn(latents_shape, device=device)\n",
    "            encoder_noise = encoder_noise.to(device)\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            latents = vae(batch[\"pixel_values\"], encoder_noise)\n",
    "\n",
    "            # Sample noise that we'll add to the latents -> it is done inside the add noise method\n",
    "            # noise = torch.randn_like(latents)\n",
    "            \n",
    "            bsz = latents.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image and text\n",
    "            timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device)\n",
    "            timesteps = timesteps.long()\n",
    "            text_timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device)\n",
    "            text_timesteps = text_timesteps.long()\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep (this is the forward diffusion process)\n",
    "            noisy_latents, image_noise = ddpm.add_noise(latents, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])\n",
    "\n",
    "            # Add noise to the text query according to the noise magnitude at each timestep\n",
    "            noisy_text_query, text_noise = ddpm.add_noise(encoder_hidden_states, text_timesteps)\n",
    "\n",
    "            image_time_embeddings = get_time_embedding(timesteps, is_image=True).to(device)\n",
    "            text_time_embeddings = get_time_embedding(timesteps, is_image=False).to(device)\n",
    "            \n",
    "            # take average and normalize the text time embeddings\n",
    "            average_noisy_text_query = noisy_text_query.mean(dim=1)\n",
    "            text_query = F.normalize(average_noisy_text_query, p=2, dim=-1)\n",
    "\n",
    "            # Target for the model is the noise that was added to the latents and the text query\n",
    "            image_target = image_noise\n",
    "            text_target = text_query\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            image_pred, text_pred = unet(noisy_latents, encoder_hidden_states, image_time_embeddings, text_time_embeddings, text_query)\n",
    "\n",
    "            image_loss = F.mse_loss(image_pred.float(), image_target.float(), reduction=\"mean\")\n",
    "            text_loss = F.mse_loss(text_pred.float(), text_target.float(), reduction=\"mean\")\n",
    "            \n",
    "            train_loss += image_loss + Lambda * text_loss\n",
    "\n",
    "            # Backpropagate\n",
    "            loss = image_loss + Lambda * text_loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            # lr_scheduler.step() # maybe linear scheduler can be added\n",
    "\n",
    "            if global_step % save_steps == 0:\n",
    "                # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "                if checkpoints_total_limit is not None:\n",
    "                    checkpoints = os.listdir(output_dir)\n",
    "                    checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n",
    "                    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "\n",
    "                    # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n",
    "                    if len(checkpoints) >= checkpoints_total_limit:\n",
    "                        num_to_remove = len(checkpoints) - checkpoints_total_limit + 1\n",
    "                        removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "\n",
    "                        print(\n",
    "                            f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n",
    "                        )\n",
    "                        print(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n",
    "\n",
    "                        for removing_checkpoint in removing_checkpoints:\n",
    "                            removing_checkpoint = os.path.join(output_dir, removing_checkpoint)\n",
    "                            os.remove(removing_checkpoint)\n",
    "\n",
    "                save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "\n",
    "                # Save model state and optimizer state\n",
    "                torch.save({\n",
    "                    'model_state_dict': unet.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, save_path)\n",
    "\n",
    "                print(f\"Saved state to {save_path}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(\"step_loss:\", loss.detach().item())\n",
    "            \n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        print(\"Average loss over epoch:\", train_loss / (step + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_train_epochs, device=\"cpu\", save_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_loader\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "tokenizer = CLIPTokenizer(\"./data/vocab.json\", merges_file=\"./data/merges.txt\")\n",
    "model_file = \"./data/v1-5-pruned.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "## TEXT TO IMAGE\n",
    "\n",
    "# prompt = \"A dog with sunglasses, wearing comfy hat, looking at camera, highly detailed, ultra sharp, cinematic, 100mm lens, 8k resolution.\"\n",
    "prompt = \"Two students suffering from completing the generative model course project, highly detailed, ultra sharp, cinematic, 100mm lens, 8k resolution.\"\n",
    "uncond_prompt = \"\"  # Also known as negative prompt\n",
    "do_cfg = False\n",
    "cfg_scale = 8  # min: 1, max: 14\n",
    "\n",
    "## SAMPLER\n",
    "\n",
    "sampler = \"ddpm\"\n",
    "num_inference_steps = 50\n",
    "seed = 42\n",
    "\n",
    "output_image = pipeline.generate(\n",
    "    prompt=prompt,\n",
    "    uncond_prompt=uncond_prompt,\n",
    "    input_image=None,\n",
    "    strength=0.9,\n",
    "    do_cfg=do_cfg,\n",
    "    cfg_scale=cfg_scale,\n",
    "    sampler_name=sampler,\n",
    "    n_inference_steps=num_inference_steps,\n",
    "    seed=seed,\n",
    "    models=models,\n",
    "    device=DEVICE,\n",
    "    idle_device=\"cpu\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Combine the input image and the output image into a single image.\n",
    "Image.fromarray(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT REMARK** For the purpose of validating the model's training process, we have opted to utilize a representative dummy dataset. This dataset, while not the actual one, mirrors the format of the original CC3M dataset. It's worth noting that the CC3M dataset is considerably large, with a total size of approximately 430 GB. Given this substantial size, and considering our resource constraints, we have determined that it would not be feasible to complete the training process prior to the deadline for the first version, which is set for May 5. However, we have devised a plan to ensure that the training process is completed by the subsequent deadline for the second version, scheduled for May 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the implementation of the model as described in the research paper, we encountered several areas of ambiguity that necessitated the formulation of certain assumptions. These assumptions, which guided our implementation, are detailed as follows:\n",
    "\n",
    "1) The paper did not provide explicit information regarding whether the transformers employed for the image-to-text alignment task were part of a separate architecture or integrated within the proposed UNet middle blocks. Given this lack of clarity, our implementation treats the transformers as a distinct architecture.\n",
    "\n",
    "2) The paper did not specify which blocks were to be modified to incorporate the dual stream deep fusion blocks. In our implementation, we have chosen to integrate these blocks into both the downsample and middle blocks of the original UNet, also known as Stable Diffusion. Notably, we did not apply these changes to the upsample layers, as they are not utilized in the image-to-text alignment task.\n",
    "\n",
    "3) The term 'text query' was used in the paper without a clear definition. In our interpretation, we have chosen to represent the text query as the normalized average of the output from the text encoder.\n",
    "\n",
    "4) The paper did not provide a clear methodology for the concatenation of the hidden latent image and the output of the fully connected layer. In our implementation, we expanded the output of the fully connected layer from a shape of (Batch Size, Channels) to (Batch Size, Channels, Height, Width), enabling its concatenation with the latent image, which also has a shape of (Batch Size, Channels, Height, Width).\n",
    "\n",
    "5) The paper did not provide explicit instructions on how the fully connected layer projects the text query back into the text embedding space. We assumed that it generates an output with a shape of (Batch Size, Width * Height, 768). We then computed the normalized average of this output along dimension 1, resulting in an output of shape (Batch Size, 768). This output serves as the hidden text query that is input to the next layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
