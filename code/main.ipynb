{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paper Name:** DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability\n",
    "\n",
    "**Link:** https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_DiffDis_Empowering_Generative_Diffusion_Model_with_Cross-Modal_Discrimination_Capability_ICCV_2023_paper.pdf\n",
    "\n",
    "**Project Members:**\n",
    "Furkan Genç,\n",
    "Barış Sarper Tezcan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Summary\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Recently, large-scale diffusion models, e.g., Stable Diffusion and DallE2, have shown remarkable results on image synthesis. On the other hand, large-scale cross-modal pretrained models (e.g., CLIP, ALIGN, and FILIP) are competent for various downstream tasks by learning to align vision and language embeddings. This paper explores the possibility of jointly modeling generation and discrimination. Specifically, the authors propose DiffDis to unify the cross-modal generative and discriminative pretraining into one single framework under the diffusion process.\n",
    "\n",
    "DiffDis first formulates the image-text discriminative problem as a generative diffusion process of the text embedding from the text encoder conditioned on the image. Then, the authors propose a novel dual-stream network architecture, which fuses the noisy text embedding with the knowledge of latent images from different scales for image-text discriminative learning. Moreover, the generative and discriminative tasks can efficiently share the image-branch network structure in the multi-modality model. Benefiting from diffusion-based unified training, DiffDis achieves both better generation ability and cross-modal semantic alignment in one architecture.\n",
    "\n",
    "Experimental results show that DiffDis outperforms single-task models on both the image generation and the image-text discriminative tasks, e.g., 1.65% improvement on average accuracy of zero-shot classification over 12 datasets and 2.42 improvement on FID of zero-shot image synthesis.\n",
    "\n",
    "The contributions of the paper can be summarized as:\n",
    "\n",
    "- The authors propose DiffDis to explore a unified vision-language diffusion model for both multi-modality generation and discrimination tasks.\n",
    "- DiffDis reformulates the image-text discriminative problem by utilizing a generative diffusion process of the text embeddings conditioned on input images.\n",
    "- A dual-stream network architecture and a diffusion-based unified training paradigm are proposed for jointly training the generative and discriminative tasks.\n",
    "- Extensive experiments demonstrate that DiffDis outperforms single-task models, achieving a 1.65% improvement on average zero-shot classification accuracy across 12 datasets and a 2.42 improvement on FID of text-guided image generation.\n",
    "- Additionally, DiffDis outperforms CLIP, with a 4.7% improvement on average zero-shot classification accuracy across 12 datasets and a 14.5% improvement on average R@1 of image-text retrieval tasks on Flickr30k and MSCOCO.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "#### Unified Model Architecture\n",
    "\n",
    "![Figure 1. Overall model architecture of DiffDis](../topic_summary_images/bayes_net.png)\n",
    "\n",
    "##### Feature Extraction Modules\n",
    "\n",
    "As shown in the figure, DiffDis includes an image encoder to obtain the latent image z and a text encoder to obtain the text condition c and text query e.\n",
    "\n",
    "- **Image Encoder**: \n",
    "  - The image encoder V of an autoencoder converts the image x from RGB space to its latent representation z. This improves training efficiency and supports high-resolution image synthesis.\n",
    "\n",
    "- **Text Encoder**: \n",
    "  - The text encoder T encodes the text prompt y into the text condition c and text query e.\n",
    "  - The text condition $ c \\in \\mathbb{R}^{L \\times d_y} $ is the token-wise representation of the text prompt y.\n",
    "  - The text query $ e \\in \\mathbb{R}^{1 \\times d_y} $ is the normalized global representation of the text prompt y.\n",
    "\n",
    "The UNet structure receives three inputs: latent image z, text condition c, and text query e.\n",
    "\n",
    "- **Text-Conditional Image Generation Task**:\n",
    "  - The model uses the noisy latent image $ z_t $ and text condition c as input to predict the noise $ \\epsilon_z $ added to the latent image z.\n",
    "\n",
    "- **Diffusion-Based Image-Text Alignment Learning**:\n",
    "  - The model uses the latent image z and noisy text query $ e_t $ as input to predict the original clean text query e. It can also predict the noise $ \\epsilon_e $ added to the text query $ e $ for alignment learning.\n",
    "\n",
    "The model includes a unique transformer with M transformer blocks and a linear predictor:\n",
    "\n",
    "- **Transformer**:\n",
    "  - Follows the middle block of the UNet to obtain more semantic information.\n",
    "  - The transformer's input is the concatenation of the flattened image's feature map and the text query outputted from the middle block of the UNet.\n",
    "\n",
    "- **Linear Predictor**:\n",
    "  - Fed by the text query token and predicts the original clean text query e.\n",
    "\n",
    "This unified approach allows for efficient training and effective handling of both generative and discriminative tasks within a single architecture.\n",
    "\n",
    "#### Dual-stream Deep Fusion Attention Block\n",
    "\n",
    "The authors modify the architecture of attention blocks in UNet to better unify the generative and discriminative tasks with different inputs. The previous stable diffusion model directly uses K transformer decoder blocks to inject text condition information into the latent image via a cross-attention mechanism. The proposed dual-stream deep fusion block enhances this process by better fusing the latent image knowledge into the text query for cross-modal alignment learning.\n",
    "\n",
    "![Dual-stream Deep Fusion Block](path_to_image.png)\n",
    "\n",
    "##### Key Components:\n",
    "\n",
    "- **Fully Connected Layer**:\n",
    "  - Projects the text query embedding into the same dimension as the hidden latent image space.\n",
    "  \n",
    "- **Concatenation and Transformer Blocks**:\n",
    "  - The concatenated projected text query and the hidden latent image $ h_{z,l-1} $ outputted by the last block pass through K transformer blocks.\n",
    "  - Each transformer block includes modality-specific feedforward neural networks (FFN) for text and image.\n",
    "  - The text query skips the cross-attention layer within these transformer blocks.\n",
    "\n",
    "- **Projection Back to Text Embedding Space**:\n",
    "  - Following the transformer blocks, the concatenation of the text query and image hidden representations is separated.\n",
    "  - The text query is projected back to the text embedding space using a fully-connected layer.\n",
    "\n",
    "- **Cross-block Skip Connection**:\n",
    "  - A skip connection is built from the noised text query $ e_t $ with the time condition to the hidden text query $ h_{e,l-1} $ outputted by the last block.\n",
    "  - The hidden text query $ h_{e,l-1} $ is initialized to zero.\n",
    "\n",
    "This dual-stream deep fusion attention block allows for more effective cross-modal alignment learning by incorporating both image and text information in a unified manner.\n",
    "\n",
    "### Task Reformulation\n",
    "\n",
    "#### Basic Notations\n",
    "\n",
    "The image-text dataset is denoted as $ D = \\{x_i, y_i\\}_{i=1}^N $, where $ x_i $ and $ y_i $ denote the $ i $-th image and text, respectively. $ N $ is the total number of image-text pairs. As shown in Fig. 2, DiffDis consists of an image encoder $ V $ and a text encoder $ T $. Additionally, $ \\Phi_u $ and $ \\Phi_\\theta $ represent the UNet model used for text-conditional image generation, and the encoder part of the UNet model with additional transformer blocks for image-text alignment learning, respectively.\n",
    "\n",
    "#### Diffusion-based Text-conditioned Image Generation\n",
    "\n",
    "The generative part of DiffDis aims to generate images conditioned on input text prompts. Following standard diffusion models, DiffDis generates image samples by gradually removing the noise from a random Gaussian noise signal over a finite number of steps. The diffusion model is trained by adding and predicting different levels of noise on the image in the opposite direction of the sampling process.\n",
    "\n",
    "**Training Procedure**:\n",
    "\n",
    "The diffusion process of an image can be represented as a parameterized Markov chain, which adds $ T $ steps of random Gaussian noise $ \\epsilon $ to gradually convert the original image $ x_0 $ to a random Gaussian distribution $ x_T $. Utilizing the latent image $ z = V(x) \\in \\mathbb{R}^{H \\times W \\times d_x} $ outputted by the image encoder $ V $ instead of the RGB space of image $ x $ as the input signal, the diffusion-based text-conditional image generation loss $ \\mathcal{L}_{IG} $, which aims to predict the added Gaussian noise, can be formulated as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{IG} = \\mathbb{E}_{\\mathcal{V}(x), \\epsilon_z \\sim \\mathcal{N}(0, I), t_z}\\left [ \\| \\epsilon_z - \\Phi_u(z_t, t_z, c) \\|^2 \\right ]\n",
    "$$\n",
    "\n",
    "where the text condition $ c = T(y) \\in \\mathbb{R}^{L \\times d_y} $ denotes the token-wise representations of the text prompt $ y $. $ L $ and $ d_y $ represent the context length and embedding dimensions of the outputted token embeddings, respectively.\n",
    "\n",
    "**Inference Procedure**:\n",
    "\n",
    "During the inference (sampling) process, starting from a random Gaussian noise $ z_T \\sim \\mathcal{N}(0, I) $, the diffusion process is reversed, and the predicted noise $ \\Phi_u(z_t, t_z, c) $ is gradually removed to obtain the sampled latent image after finite steps. An image decoder $ D $ is used to convert the sampled latent image back to RGB space $ \\hat{x} = D(\\hat{z}_0) $. The DDIM sampler and classifier-free guidance are employed.\n",
    "\n",
    "#### Diffusion-based Image-text Alignment Pretraining\n",
    "\n",
    "Previous methods perform image-text alignment pretraining by aligning the visual feature and textual feature in a common semantic space, where the positive image-text pairs are pulled towards each other and the negative image-text pairs are pushed against each other. The image-text contrastive (ITC) loss, like in CLIP, first calculates the cosine similarity of normalized visual feature $ v_i $ and textual feature $ e_j $ to measure the relevance of each image and text:\n",
    "\n",
    "$$\n",
    "s_{i,j}^y = s_{i,j}^x = v_i^\\top e_j\n",
    "$$\n",
    "\n",
    "where $ s_{i,j}^x $, $ s_{i,j}^y $ denote the image-to-text similarity and the text-to-image similarity, respectively. $ v_i $ is the global representation of the image $ x_i $ and $ e_j $ is the global representation of the text $ y_i $.\n",
    "\n",
    "To reformulate this image-text alignment problem into a diffusion process, the authors propose the denoising diffusion-based image-text alignment, which treats the latent image $ z $ as the condition and learns the distribution of the corresponding text embedding $ e \\in \\mathbb{R}^{1 \\times d_y} $. The diffusion process of $ e $ can be formulated as:\n",
    "\n",
    "$$\n",
    "e_t = \\gamma \\sqrt {\\bar {\\alpha }_t} e_0 + \\sqrt {1-\\bar {\\alpha }_t} \\epsilon _e, \\\\\n",
    "\\alpha _t = 1-\\beta _t, \\bar {\\alpha }_t = \\prod _{j=0}^t \\alpha _j,\n",
    "$$\n",
    "\n",
    "where $ \\beta_t $ is used to control the strength of added noise for timestep $ t_e $. The $ \\gamma \\in (0, 1] $ is the scale factor to scale the text embedding $ e_0 $.\n",
    "\n",
    "In contrast to the image generation task, the diffusion model $ \\Phi_\\theta $ is trained to estimate the original clean text query $ \\hat{e}_0 = \\Phi_\\theta(e_t, t_e, z) $. Note that $ \\Phi_\\theta $ can also be a noise prediction model to predict the noise $ \\epsilon_e $.\n",
    "\n",
    "Then, the diffusion-based image-text alignment objective is to minimize the distance between $ \\hat{e}_0 $ and $ e $. The cosine similarity between $ \\hat{e}_0 $ and $ e $ is calculated as:\n",
    "\n",
    "$$\n",
    "s = \\hat {e}_0 ^\\top e\n",
    "$$\n",
    "\n",
    "The diffusion-based image-text alignment loss $ \\mathcal{L}_{ITA} $ can be calculated as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{ITA} =-\\frac {1}{2B}\\sum ^{B}_{i=1}\\left [\\log \\frac {\\exp (s_{i,i}^x)}{ \\sum _{j} \\exp (s_{i,j}^x)} + \\log \\frac {\\exp (s_{i,i}^y)}{ \\sum _{j} \\exp (s_{j, i}^y)}\\right ]\n",
    "$$\n",
    "\n",
    "where $ B $ denotes the batch size, and $ \\mathcal{L}_{ITA} $ is calculated for each diffusion step during training.\n",
    "\n",
    "### Diffusion-based Unified Training\n",
    "\n",
    "In this section, the authors introduce a training paradigm to unify diffusion-based image generation training and cross-modal alignment learning into a single framework. The whole training algorithm is illustrated in Algorithm 1.\n",
    "\n",
    "To alternatively feed the required inputs and conditions when performing diffusion-based generation and alignment tasks, a masking mechanism is introduced to erase unnecessary input. The input space includes the latent image $ z $ outputted by the image encoder, text condition $ c $, and text query $ e $ outputted by the text encoder.\n",
    "\n",
    "- **Image Generation Task**:\n",
    "  - Mask the text query $ e $.\n",
    "  - Feed the noisy latent image $ z_t $ with time step $ t $ and text condition $ c $ into the following UNet model.\n",
    "\n",
    "- **Image-Text Alignment Learning**:\n",
    "  - Mask the text condition $ c $ to avoid leaking textual information.\n",
    "  - Feed the noisy text query $ e_t $ with time step $ t $ and the latent image $ z $ as the condition.\n",
    "\n",
    "Considering the image generation loss $ \\mathcal{L}_{IG} $ from Eq. 4 and the diffusion-based image-text alignment loss $ \\mathcal{L}_{ITA} $ from Eq. 8, the total loss of DiffDis $ \\mathcal{L}_{Total} $ can be calculated as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{Total} = \\mathcal{L}_{IG} + \\lambda \\mathcal{L}_{ITA}\n",
    "$$\n",
    "\n",
    "where $ \\lambda $ denotes the weight factor.\n",
    "\n",
    "#### Algorithm 1: Diffusion-based Unified Training\n",
    "\n",
    "**Input:** Image $x$, Text $y$, Image Encoder $\\mathcal{V}$, Text Encoder $\\mathcal{T}$, Timestep $T$.\n",
    "\n",
    "1. repeat\n",
    "2.     $c, e = \\mathcal{T}(y)$  # Get text condition and text query\n",
    "3.     $z = \\mathcal{V}(x)$\n",
    "4.     $t_z, t_e \\sim \\text{Uniform}(\\{1, \\ldots, T\\})$\n",
    "5.     $\\epsilon_z, \\epsilon_e \\sim \\mathcal{N}(0, I)$\n",
    "6.     Mask $e$ and calculate $\\mathcal{L}_{IG}$ based on Eq. 4.\n",
    "7.     Mask $c$ and calculate $\\mathcal{L}_{ITA}$ based on Eq. 8.\n",
    "8.     Calculate total loss $\\mathcal{L}$ based on Eq. 9\n",
    "9.     Take gradient descent step on $\\nabla_{u,\\theta} \\mathcal{L}_{Total}$\n",
    "10. until converged\n",
    "\n",
    "#### Algorithm 2: Text-conditional Image Generation Sampling\n",
    "**Input:** Text $y$, Text Encoder $\\mathcal{T}$, Image Decoder $\\mathcal{D}$, Timestep $T$, Noise Schedule $\\{\\beta_t\\}_{t=1}^T$, Classifier-free guidance scale $w$.\n",
    "\n",
    "1. $z_T \\sim \\mathcal{N}(0, I)$\n",
    "2. $c = \\mathcal{T}(y)$\n",
    "3. $\\alpha_t = 1 - \\beta_t, \\bar{\\alpha}_t = \\prod_{k=1}^t \\alpha_t$\n",
    "4. for $t = T, \\ldots, 1$  # For simplicity, $t$ stands for $t_z$\n",
    "5.     $\\hat{\\epsilon}_z = (1 + w) \\Phi_u(z_t, t, c) - w \\Phi_u(z_t, t)$\n",
    "6.     $z_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\left( z_t - \\sqrt{\\frac{1 - \\bar{\\alpha}_t}{\\bar{\\alpha}_t}} \\hat{\\epsilon}_z \\right) + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\hat{\\epsilon}_z$\n",
    "7. end for\n",
    "8. return $\\mathcal{D}(z_0)$\n",
    "\n",
    "#### Algorithm 3: Image-text Alignment Inference\n",
    "**Input:** Image $x$, Text $y'$ of Downstream Task, Image Encoder $\\mathcal{V}$, Text Encoder $\\mathcal{T}$, Timestep $T$, Noise Schedule $\\{\\beta_t\\}_{t=1}^T$, Classifier-free guidance scale $w$.\n",
    "\n",
    "1. $e_T \\sim \\mathcal{N}(0, I)$\n",
    "2. $z = \\mathcal{V}(x)$\n",
    "3. $\\alpha_t = 1 - \\beta_t, \\bar{\\alpha}_t = \\prod_{k=1}^t \\alpha_t$\n",
    "4. for $t = T, \\ldots, 1$  # For simplicity, $t$ stands for $t_e$\n",
    "5.     $\\hat{e}_0 = (1 + w) \\Phi_\\theta(e_t, t, z) - w \\Phi_\\theta(e_t, t)$\n",
    "6.     $\\hat{\\epsilon}_e = \\frac{e_t - \\sqrt{\\bar{\\alpha}_t} \\hat{e}_0}{\\sqrt{1 - \\bar{\\alpha}_t}}$\n",
    "7.     $e_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\left( e_t - \\sqrt{\\frac{1 - \\bar{\\alpha}_t}{\\bar{\\alpha}_t}} \\hat{\\epsilon}_e \\right) + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\hat{\\epsilon}_e$\n",
    "8. end for\n",
    "9. $e_0 = \\frac{e_0}{\\|e_0\\|}$  # L2 Normalize\n",
    "10. $e = \\mathcal{T}(y')$  # Extract Text Embedding\n",
    "11. $e = \\frac{e}{\\|e\\|}$  # L2 Normalize\n",
    "12. Perform the similarity $e_0^\\top e$ on downstream tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the constants \n",
    "WIDTH = 512\n",
    "HEIGHT = 512\n",
    "LATENTS_WIDTH = WIDTH // 8\n",
    "LATENTS_HEIGHT = HEIGHT // 8\n",
    "BATCH_SIZE = 1\n",
    "root_dir = \"../dataset/cc3m/train\" # TODO: change root_dir with the path to the dataset according to your setup\n",
    "\n",
    "# training parameters\n",
    "num_train_epochs = 6\n",
    "Lambda = 1.0\n",
    "save_steps = 1000\n",
    "\n",
    "# optimizer parameters\n",
    "learning_rate = 1e-5\n",
    "discriminative_learning_rate = 1e-4  # New learning rate for discriminative tasks\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_weight_decay = 1e-4\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# IMAGE TO TEXT\n",
    "test_dataset = \"CIFAR10\"  # Set to \"CIFAR100\" to use CIFAR100 dataset\n",
    "\n",
    "# output directory\n",
    "train_output_dir = \"../results/output_1\"\n",
    "test_output_dir = \"../results/\" + test_dataset\n",
    "inference_output_dir = \"../results/text_to_image/output_1/last\"\n",
    "\n",
    "# Load the models\n",
    "model_file = \"data/v1-5-pruned.ckpt\"  \n",
    "train_unet_file = None  # Set to None to finetune from scratch, if specified, the diffusion model will be loaded from this file\n",
    "test_unet_file = \"../results/output_1/last.pt\" \n",
    "inference_unet_file = \"../results/output_1/last.pt\"\n",
    "\n",
    "# EMA parameters\n",
    "use_ema = False  # Set to True to use EMA\n",
    "ema_decay = 0.9999\n",
    "warmup_steps = 1000\n",
    "\n",
    "# TEXT TO IMAGE\n",
    "prompt1 = \"A river with boats docked and houses in the background\"\n",
    "prompt2 = \"A piece of chocolate swirled cake on a plate\"\n",
    "prompt3 = \"A large bed sitting next to a small Christmas Tree surrounded by pictures\"\n",
    "prompt4 = \"A bear searching for food near the river\"\n",
    "prompts = [prompt1, prompt2, prompt3, prompt4]\n",
    "uncond_prompt = \"\"  # Also known as negative prompt\n",
    "do_cfg = True\n",
    "cfg_scale = 3  # min: 1, max: 14\n",
    "num_samples = 1\n",
    "\n",
    "# SAMPLER\n",
    "sampler = \"ddpm\"\n",
    "num_inference_steps = 50\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from ddpm import DDPMSampler\n",
    "from pipeline import get_time_embedding\n",
    "from dataloader import train_dataloader\n",
    "import model_loader\n",
    "import time\n",
    "from diffusion import TransformerBlock, UNet_Transformer  # Ensure these are correctly imported\n",
    "\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# Set the device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the models\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "ddpm = DDPMSampler(generator=None)\n",
    "\n",
    "if train_unet_file is not None:\n",
    "    # Load the UNet model\n",
    "    print(f\"Loading UNet model from {train_unet_file}\")\n",
    "    models['diffusion'].load_state_dict(torch.load(train_unet_file)['model_state_dict'])\n",
    "    if 'best_loss' in torch.load(train_unet_file):\n",
    "        best_loss = torch.load(train_unet_file)['best_loss']\n",
    "        best_step = torch.load(train_unet_file)['best_step']\n",
    "        last_loss = torch.load(train_unet_file)['last_loss']\n",
    "        last_step = torch.load(train_unet_file)['last_step']\n",
    "    else:\n",
    "        best_loss = float('inf')\n",
    "        best_step = 0\n",
    "        last_loss = 0.0\n",
    "        last_step = 0\n",
    "else:\n",
    "    best_loss = float('inf')\n",
    "    best_step = 0\n",
    "    last_loss = 0.0\n",
    "    last_step = 0\n",
    "\n",
    "# TEXT TO IMAGE\n",
    "tokenizer = CLIPTokenizer(\"./data/vocab.json\", merges_file=\"./data/merges.txt\")\n",
    "\n",
    "# Disable gradient computations for the models['encoder'], DDPM, and models['clip'] models\n",
    "for param in models['encoder'].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in models['clip'].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set the models['encoder'] and models['clip'] to eval mode\n",
    "models['encoder'].eval()\n",
    "models['clip'].eval()\n",
    "\n",
    "# Separate parameters for discriminative tasks\n",
    "discriminative_params = []\n",
    "non_discriminative_params = []\n",
    "\n",
    "for name, param in models['diffusion'].named_parameters():\n",
    "    if isinstance(getattr(models['diffusion'], name.split('.')[0], None), (TransformerBlock, UNet_Transformer)):\n",
    "        discriminative_params.append(param)\n",
    "    else:\n",
    "        non_discriminative_params.append(param)\n",
    "\n",
    "# AdamW optimizer with separate learning rates\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': non_discriminative_params, 'lr': learning_rate},\n",
    "    {'params': discriminative_params, 'lr': discriminative_learning_rate}\n",
    "], betas=(adam_beta1, adam_beta2), weight_decay=adam_weight_decay, eps=adam_epsilon)\n",
    "\n",
    "if train_unet_file is not None:\n",
    "    print(f\"Loading optimizer state from {train_unet_file}\")\n",
    "    optimizer.load_state_dict(torch.load(train_unet_file)['optimizer_state_dict'])\n",
    "\n",
    "# Linear warmup scheduler for non-discriminative parameters\n",
    "def warmup_lr_lambda(current_step: int):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[\n",
    "    warmup_lr_lambda,  # Apply warmup for non-discriminative params\n",
    "    lambda step: 1.0  # Keep constant learning rate for discriminative params\n",
    "])\n",
    "\n",
    "# EMA setup\n",
    "if use_ema:\n",
    "    ema_unet = torch.optim.swa_utils.AveragedModel(models['diffusion'], avg_fn=lambda averaged_model_parameter, model_parameter, num_averaged: ema_decay * averaged_model_parameter + (1 - ema_decay) * model_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_train_epochs, device=\"cuda\", save_steps=1000):\n",
    "    global best_loss, best_step, last_loss, last_step\n",
    "\n",
    "    if train_unet_file is not None:\n",
    "        first_epoch = last_step // len(train_dataloader)\n",
    "        global_step = last_step + 1\n",
    "    else:\n",
    "        first_epoch = 0\n",
    "        global_step = 0\n",
    "\n",
    "    accumulator = 0\n",
    "\n",
    "    # Move models to the device\n",
    "    models['encoder'].to(device)\n",
    "    models['clip'].to(device)\n",
    "    models['diffusion'].to(device)\n",
    "    if use_ema:\n",
    "        ema_unet.to(device)\n",
    "\n",
    "    num_train_epochs = tqdm(range(first_epoch, num_train_epochs), desc=\"Epoch\")\n",
    "    for epoch in num_train_epochs:\n",
    "        train_loss = 0.0\n",
    "        num_train_steps = len(train_dataloader)\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Extract images and texts from batch\n",
    "            images = batch[\"pixel_values\"]\n",
    "            texts = batch[\"input_ids\"]\n",
    "\n",
    "            # Move batch to the device\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            # Encode images to latent space\n",
    "            encoder_noise = torch.randn(images.shape[0], 4, LATENTS_HEIGHT, LATENTS_WIDTH).to(device)  # Shape (BATCH_SIZE, 4, 32, 32)\n",
    "            latents = models['encoder'](images, encoder_noise)\n",
    "\n",
    "            # Sample noise and timesteps for diffusion process\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "            text_timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "\n",
    "            # Add noise to latents and texts\n",
    "            noisy_latents, image_noise = ddpm.add_noise(latents, timesteps)\n",
    "            encoder_hidden_states = models['clip'](texts)\n",
    "            noisy_text_query, text_noise = ddpm.add_noise(encoder_hidden_states, text_timesteps)\n",
    "\n",
    "            # Get time embeddings\n",
    "            image_time_embeddings = get_time_embedding(timesteps, is_image=True).to(device)\n",
    "            text_time_embeddings = get_time_embedding(timesteps, is_image=False).to(device)\n",
    "            \n",
    "            # Average and normalize text time embeddings\n",
    "            average_noisy_text_query = noisy_text_query.mean(dim=1)\n",
    "            text_query = F.normalize(average_noisy_text_query, p=2, dim=-1)\n",
    "\n",
    "            # Randomly drop 10% of text and image conditions: Context Free Guidance\n",
    "            if torch.rand(1).item() < 0.1:\n",
    "                text_query = torch.zeros_like(text_query)\n",
    "            if torch.rand(1).item() < 0.1:\n",
    "                noisy_latents = torch.zeros_like(noisy_latents)\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            image_pred, text_pred = models['diffusion'](noisy_latents, encoder_hidden_states, image_time_embeddings, text_time_embeddings, text_query)\n",
    "            image_loss = F.mse_loss(image_pred.float(), image_noise.float(), reduction=\"mean\")\n",
    "            text_loss = F.mse_loss(text_pred.float(), text_query.float(), reduction=\"mean\")\n",
    "\n",
    "            loss = image_loss + Lambda * text_loss\n",
    "            train_loss += loss.item()\n",
    "            accumulator += loss.item()\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            if use_ema:\n",
    "                ema_unet.update_parameters(models['diffusion'])\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            if train_unet_file is not None and epoch == first_epoch:\n",
    "                print(f\"Step: {step+1+last_step}/{num_train_steps+last_step}   Loss: {loss.item()}   Time: {end_time - start_time}\", end=\"\\r\")\n",
    "            else:\n",
    "                print(f\"Step: {step}/{num_train_steps}   Loss: {loss.item()}   Time: {end_time - start_time}\", end=\"\\r\")\n",
    "\n",
    "            if global_step % save_steps == 0 and global_step > 0:\n",
    "                # Check if the current step's loss is the best\n",
    "                if accumulator / save_steps < best_loss:\n",
    "                    best_loss = accumulator / save_steps\n",
    "                    best_step = global_step\n",
    "                    best_save_path = os.path.join(train_output_dir, \"best.pt\")\n",
    "                    if use_ema:\n",
    "                        torch.save({\n",
    "                            'model_state_dict': models['diffusion'].state_dict(),\n",
    "                            'ema_state_dict': ema_unet.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_loss': best_loss,\n",
    "                            'best_step': best_step,\n",
    "                            'last_loss': accumulator / save_steps,\n",
    "                            'last_step': global_step\n",
    "                        }, best_save_path) \n",
    "                    else:\n",
    "                        torch.save({\n",
    "                            'model_state_dict': models['diffusion'].state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_loss': best_loss,\n",
    "                            'best_step': best_step,\n",
    "                            'last_loss': accumulator / save_steps,\n",
    "                            'last_step': global_step\n",
    "                        }, best_save_path)              \n",
    "\n",
    "                    print(f\"\\nNew best model saved to {best_save_path} with loss {best_loss}\")\n",
    "\n",
    "                # Save model and optimizer state\n",
    "                last_save_path = os.path.join(train_output_dir, f\"last.pt\")\n",
    "                if use_ema:\n",
    "                    torch.save({\n",
    "                        'model_state_dict': models['diffusion'].state_dict(),\n",
    "                        'ema_state_dict': ema_unet.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'best_loss': best_loss,\n",
    "                        'best_step': best_step,\n",
    "                        'last_loss': accumulator / save_steps,\n",
    "                        'last_step': global_step\n",
    "                    }, last_save_path)\n",
    "                else:\n",
    "                    torch.save({\n",
    "                        'model_state_dict': models['diffusion'].state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'best_loss': best_loss,\n",
    "                        'best_step': best_step,\n",
    "                        'last_loss': accumulator / save_steps,\n",
    "                        'last_step': global_step\n",
    "                    }, last_save_path)\n",
    "                    \n",
    "                print(f\"Saved state to {last_save_path}\")\n",
    "\n",
    "                # Generate samples from the model\n",
    "                for i, prompt in enumerate(prompts):\n",
    "                    # Sample images from the model\n",
    "                    output_image = pipeline.generate(\n",
    "                        prompt=prompt,\n",
    "                        uncond_prompt=uncond_prompt,\n",
    "                        input_image=None,\n",
    "                        strength=0.9,\n",
    "                        do_cfg=do_cfg,\n",
    "                        cfg_scale=cfg_scale,\n",
    "                        sampler_name=sampler,\n",
    "                        n_inference_steps=num_inference_steps,\n",
    "                        seed=seed,\n",
    "                        models=models,\n",
    "                        device=DEVICE,\n",
    "                        idle_device=DEVICE,\n",
    "                        tokenizer=tokenizer,\n",
    "                    )\n",
    "\n",
    "                    # Save the generated image\n",
    "                    output_image = Image.fromarray(output_image)\n",
    "                    output_image.save(os.path.join(train_output_dir, \"images\", \"prompt\" + str(i+1), f\"step{global_step}.png\"))\n",
    "                \n",
    "                print(f\"\\nSaved images for step {global_step}\")\n",
    "                print('Epoch: %d   Step: %d   Loss: %.5f   Best Loss: %.5f   Best Step: %d\\n' % (epoch+1, global_step, accumulator / save_steps, best_loss, best_step))\n",
    "\n",
    "                accumulator = 0.0\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        print(f\"Average loss over epoch: {train_loss / (step + 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '==> Training starts..'\n",
    "s += f'\\n\\nModel file: {model_file}'\n",
    "s += f'\\nUNet file: {train_unet_file}'\n",
    "s += f'\\nBatch size: {BATCH_SIZE}'\n",
    "s += f'\\nWidth: {WIDTH}'\n",
    "s += f'\\nHeight: {HEIGHT}'\n",
    "s += f'\\nLatents width: {LATENTS_WIDTH}'\n",
    "s += f'\\nLatents height: {LATENTS_HEIGHT}'\n",
    "s += f'\\nFirst epoch: {last_step // len(train_dataloader)}'\n",
    "s += f'\\nNumber of training epochs: {num_train_epochs}'\n",
    "s += f'\\nLambda: {Lambda}'\n",
    "s += f'\\nLearning rate: {learning_rate}'\n",
    "s += f'\\nDiscriminative learning rate: {discriminative_learning_rate}'\n",
    "s += f'\\nAdam beta1: {adam_beta1}'\n",
    "s += f'\\nAdam beta2: {adam_beta2}'\n",
    "s += f'\\nAdam weight decay: {adam_weight_decay}'\n",
    "s += f'\\nAdam epsilon: {adam_epsilon}'\n",
    "s += f'\\nUse EMA: {use_ema}'\n",
    "s += f'\\nEMA decay: {ema_decay}'\n",
    "s += f'\\nWarmup steps: {warmup_steps}'\n",
    "s += f'\\nOutput directory: {train_output_dir}'\n",
    "s += f'\\nSave steps: {save_steps}'\n",
    "s += f'\\nDevice: {DEVICE}'\n",
    "s += f'\\nSampler: {sampler}'\n",
    "s += f'\\nNumber of inference steps: {num_inference_steps}'\n",
    "s += f'\\nSeed: {seed}'\n",
    "for i, prompt in enumerate(prompts):\n",
    "    s += f'\\nPrompt {i + 1}: {prompt}'\n",
    "s += f'\\nUnconditional prompt: {uncond_prompt}'\n",
    "s += f'\\nDo CFG: {do_cfg}'\n",
    "s += f'\\nCFG scale: {cfg_scale}'\n",
    "s += f'\\n\\n'\n",
    "print(s)\n",
    "\n",
    "\n",
    "train(num_train_epochs=num_train_epochs, device=DEVICE, save_steps=save_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from ddpm import DDPMSampler\n",
    "from pipeline import get_time_embedding\n",
    "import model_loader\n",
    "import time\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# Set the device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the models\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "ddpm = DDPMSampler(generator=None)\n",
    "\n",
    "if test_unet_file is not None:\n",
    "    # Load the UNet model\n",
    "    print(f\"Loading UNet model from {test_unet_file}\")\n",
    "    if use_ema:\n",
    "        models['diffusion'].load_state_dict(torch.load(test_unet_file)['ema_state_dict'])\n",
    "    else:\n",
    "        models['diffusion'].load_state_dict(torch.load(test_unet_file)['model_state_dict'])\n",
    "\n",
    "# TEXT TO IMAGE\n",
    "tokenizer = CLIPTokenizer(\"./data/vocab.json\", merges_file=\"./data/merges.txt\")\n",
    "\n",
    "# Set the models['encoder'], models['clip'], models['diffusion'] to eval mode\n",
    "models['encoder'].eval()\n",
    "models['clip'].eval()\n",
    "models['diffusion'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device=\"cuda\"):\n",
    "    # Get the transform for the test data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((WIDTH, HEIGHT), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    # Load the CIFAR-10 dataset\n",
    "    if test_dataset == \"CIFAR10\":\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root='../dataset', train=False, download=True, transform=transform)\n",
    "\n",
    "    elif test_dataset == \"CIFAR100\":\n",
    "        testset = torchvision.datasets.CIFAR100(\n",
    "            root='../dataset', train=False, download=True, transform=transform)\n",
    "\n",
    "    print(f\"Test dataset: {test_dataset} | Number of test samples: {len(testset)}\")\n",
    "\n",
    "    # Load the test data\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Move models to the device\n",
    "    models['encoder'].to(device)\n",
    "    models['clip'].to(device)\n",
    "    models['diffusion'].to(device)\n",
    "\n",
    "    # Define the class names and tokens\n",
    "    class_names = testset.classes\n",
    "    class_tokens = []\n",
    "\n",
    "    # Tokenize class names\n",
    "    for class_name in class_names:\n",
    "        # Tokenize text\n",
    "        tokens = tokenizer.batch_encode_plus(\n",
    "            [class_name], padding=\"max_length\", max_length=77\n",
    "        ).input_ids\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long).squeeze()\n",
    "        class_tokens.append(tokens)\n",
    "\n",
    "    # Convert list of class tokens to a tensor\n",
    "    class_tokens = torch.stack(class_tokens).to(device)\n",
    "    print(f\"Class tokens shape: {class_tokens.shape}\")\n",
    "\n",
    "    # Encode class tokens with the CLIP model\n",
    "    with torch.no_grad():\n",
    "        # Encode class tokens\n",
    "        encoder_hidden_states = models['clip'](class_tokens)\n",
    "\n",
    "        # Average and normalize class embeddings\n",
    "        class_embeddings = encoder_hidden_states.mean(dim=1)\n",
    "        class_embeddings = F.normalize(class_embeddings, p=2, dim=-1)\n",
    "        print(f\"Class embeddings shape: {class_embeddings.shape}\\n\")\n",
    "    \n",
    "    # Start testing\n",
    "    test_loss = 0.0\n",
    "    num_test_steps = len(testloader)\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(testloader):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Move batch to the device\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            texts = [class_tokens[target] for target in targets]\n",
    "            \n",
    "            # Convert list of class tokens to a tensor\n",
    "            texts = torch.stack(texts).to(device)\n",
    "\n",
    "            # Encode images to latent space\n",
    "            encoder_noise = torch.randn(images.shape[0], 4, LATENTS_HEIGHT, LATENTS_WIDTH).to(device)  # Shape (BATCH_SIZE, 4, 32, 32)\n",
    "            latents = models['encoder'](images, encoder_noise)\n",
    "\n",
    "            # Sample noise and timesteps for diffusion process\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "            text_timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "\n",
    "            # Add noise to latents and texts\n",
    "            noisy_latents, image_noise = ddpm.add_noise(latents, timesteps)\n",
    "            encoder_hidden_states = models['clip'](texts)\n",
    "            noisy_text_query, text_noise = ddpm.add_noise(encoder_hidden_states, text_timesteps)\n",
    "\n",
    "            # Get time embeddings\n",
    "            image_time_embeddings = get_time_embedding(timesteps, is_image=True).to(device)\n",
    "            text_time_embeddings = get_time_embedding(timesteps, is_image=False).to(device)\n",
    "            \n",
    "            # Average and normalize text time embeddings\n",
    "            average_noisy_text_query = noisy_text_query.mean(dim=1)\n",
    "            text_query = F.normalize(average_noisy_text_query, p=2, dim=-1)\n",
    "\n",
    "            # Randomly drop 10% of text and image conditions: Context Free Guidance\n",
    "            if torch.rand(1).item() < 0.1:\n",
    "                text_query = torch.zeros_like(text_query)\n",
    "            if torch.rand(1).item() < 0.1:\n",
    "                noisy_latents = torch.zeros_like(noisy_latents)\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            _, text_pred = models['diffusion'](noisy_latents, encoder_hidden_states, image_time_embeddings, text_time_embeddings, text_query)\n",
    "                \n",
    "            # Calculate loss\n",
    "            loss = F.mse_loss(text_pred.float(), text_query.float(), reduction=\"mean\")\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate cosine similarity between the generated text query and class embeddings\n",
    "            similarities = F.cosine_similarity(text_pred.unsqueeze(1), class_embeddings.unsqueeze(0), dim=-1)\n",
    "            predicted_classes = similarities.argmax(dim=-1)\n",
    "\n",
    "            # Compare predictions with actual targets\n",
    "            correct_predictions += (predicted_classes == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(f\"Batch {batch_idx + 1}/{num_test_steps} | Loss: {loss:.4f} | Time: {end_time - start_time:.2f}s\", end=\"\\r\")\n",
    "\n",
    "    # Calculate total accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    s = f\"Accuracy: {correct_predictions}/{total_predictions} ({accuracy:.4f})\\n\"\n",
    "    s += f\"\\nTest Loss: {test_loss / num_test_steps:.4f}\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '==> Testing starts..'\n",
    "s += f'\\n\\nTest dataset: {test_dataset}'\n",
    "s += f'\\nModel file: {model_file}'\n",
    "s += f'\\nUNet file: {test_unet_file}'\n",
    "s += f'\\nBatch size: {BATCH_SIZE}'\n",
    "s += f'\\nWidth: {WIDTH}'\n",
    "s += f'\\nHeight: {HEIGHT}'\n",
    "s += f'\\nLatents width: {LATENTS_WIDTH}'\n",
    "s += f'\\nLatents height: {LATENTS_HEIGHT}'\n",
    "s += f'\\nNumber of training epochs: {num_train_epochs}'\n",
    "s += f'\\nLambda: {Lambda}'\n",
    "s += f'\\nLearning rate: {learning_rate}'\n",
    "s += f'\\nDiscriminative learning rate: {discriminative_learning_rate}'\n",
    "s += f'\\nAdam beta1: {adam_beta1}'\n",
    "s += f'\\nAdam beta2: {adam_beta2}'\n",
    "s += f'\\nAdam weight decay: {adam_weight_decay}'\n",
    "s += f'\\nAdam epsilon: {adam_epsilon}'\n",
    "s += f'\\nUse EMA: {use_ema}'\n",
    "s += f'\\nEMA decay: {ema_decay}'\n",
    "s += f'\\nWarmup steps: {warmup_steps}'\n",
    "s += f'\\nOutput directory: {test_output_dir}'\n",
    "s += f'\\nSave steps: {save_steps}'\n",
    "s += f'\\nDevice: {DEVICE}'\n",
    "s += f'\\nSampler: {sampler}'\n",
    "s += f'\\nNumber of inference steps: {num_inference_steps}'\n",
    "s += f'\\nSeed: {seed}'\n",
    "for i, prompt in enumerate(prompts):\n",
    "    s += f'\\nPrompt {i + 1}: {prompt}'\n",
    "s += f'\\nUnconditional prompt: {uncond_prompt}'\n",
    "s += f'\\nDo CFG: {do_cfg}'\n",
    "s += f'\\nCFG scale: {cfg_scale}'\n",
    "s += f'\\n\\n'\n",
    "print(s)\n",
    "\n",
    "test(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import model_loader\n",
    "import time\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer\n",
    "from IPython.display import display\n",
    "\n",
    "# Set the device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the models\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "if inference_unet_file is not None:\n",
    "    # Load the UNet model\n",
    "    print(f\"Loading UNet model from {inference_unet_file}\")\n",
    "    models['diffusion'].load_state_dict(torch.load(inference_unet_file)['model_state_dict'])\n",
    "\n",
    "# TEXT TO IMAGE\n",
    "tokenizer = CLIPTokenizer(\"./data/vocab.json\", merges_file=\"./data/merges.txt\")\n",
    "\n",
    "# Generate samples from the model\n",
    "for i, prompt in enumerate(prompts):\n",
    "    for j in range(num_samples):\n",
    "        start = time.time()\n",
    "\n",
    "        # Sample images from the model\n",
    "        output_image = pipeline.generate(\n",
    "            prompt=prompt,\n",
    "            uncond_prompt=uncond_prompt,\n",
    "            input_image=None,\n",
    "            strength=0.9,\n",
    "            do_cfg=do_cfg,\n",
    "            cfg_scale=cfg_scale,\n",
    "            sampler_name=sampler,\n",
    "            n_inference_steps=num_inference_steps,\n",
    "            seed=seed,\n",
    "            models=models,\n",
    "            device=DEVICE,\n",
    "            idle_device=DEVICE,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "        print(f\"PROMPT {i+1} - SAMPLE {j+1} - TIME: {end - start:.2f}s\\n\")\n",
    "\n",
    "        # Save the generated image\n",
    "        output_image = Image.fromarray(output_image)\n",
    "        \n",
    "        # Display the generated image\n",
    "        display(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT REMARK** For the purpose of validating the model's training process, we have opted to utilize a representative dummy dataset. This dataset, while not the actual one, mirrors the format of the original CC3M dataset. It's worth noting that the CC3M dataset is considerably large, with a total size of approximately 430 GB. Given this substantial size, and considering our resource constraints, we have determined that it would not be feasible to complete the training process prior to the deadline for the first version, which is set for May 5. However, we have devised a plan to ensure that the training process is completed by the subsequent deadline for the second version, scheduled for May 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the implementation of the model as described in the research paper, we encountered several areas of ambiguity that necessitated the formulation of certain assumptions. These assumptions, which guided our implementation, are detailed as follows:\n",
    "\n",
    "1) The paper did not provide explicit information regarding whether the transformers employed for the image-to-text alignment task were part of a separate architecture or integrated within the proposed UNet middle blocks. Given this lack of clarity, our implementation treats the transformers as a distinct architecture.\n",
    "\n",
    "2) The paper did not specify which blocks were to be modified to incorporate the dual stream deep fusion blocks. In our implementation, we have chosen to integrate these blocks into both the downsample and middle blocks of the original UNet, also known as Stable Diffusion. Notably, we did not apply these changes to the upsample layers, as they are not utilized in the image-to-text alignment task.\n",
    "\n",
    "3) The term 'text query' was used in the paper without a clear definition. In our interpretation, we have chosen to represent the text query as the normalized average of the output from the text encoder.\n",
    "\n",
    "4) The paper did not provide a clear methodology for the concatenation of the hidden latent image and the output of the fully connected layer. In our implementation, we expanded the output of the fully connected layer from a shape of (Batch Size, Channels) to (Batch Size, Channels, Height, Width), enabling its concatenation with the latent image, which also has a shape of (Batch Size, Channels, Height, Width).\n",
    "\n",
    "5) The paper did not provide explicit instructions on how the fully connected layer projects the text query back into the text embedding space. We assumed that it generates an output with a shape of (Batch Size, Width * Height, 768). We then computed the normalized average of this output along dimension 1, resulting in an output of shape (Batch Size, 768). This output serves as the hidden text query that is input to the next layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
