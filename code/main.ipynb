{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paper Name:** DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability\n",
    "\n",
    "**Link:** https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_DiffDis_Empowering_Generative_Diffusion_Model_with_Cross-Modal_Discrimination_Capability_ICCV_2023_paper.pd\n",
    "\n",
    "**Project Members:**\n",
    "Furkan Genç,\n",
    "Barış Sarper Tezcan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the constants \n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "LATENTS_WIDTH = WIDTH // 8\n",
    "LATENTS_HEIGHT = HEIGHT // 8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# training parameters\n",
    "first_epoch = 0\n",
    "num_train_epochs = 6\n",
    "Lambda = 1.0\n",
    "\n",
    "# optimizer parameters\n",
    "learning_rate = 1e-5\n",
    "discriminative_learning_rate = 1e-4  # New learning rate for discriminative tasks\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_weight_decay = 1e-4\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# checkpoint parameters\n",
    "output_dir = \"output\"\n",
    "save_steps = 10000\n",
    "max_train_steps = 1000000\n",
    "\n",
    "# EMA parameters\n",
    "ema_decay = 0.9999\n",
    "warmup_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from ddpm import DDPMSampler\n",
    "from pipeline import get_time_embedding\n",
    "from dataloader import train_dataloader\n",
    "import model_loader\n",
    "import time\n",
    "from diffusion import TransformerBlock, UNet_Transformer  # Ensure these are correctly imported\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"  # Force CPU for now\n",
    "\n",
    "model_file = \"./data/v1-5-pruned.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "vae = models['encoder']\n",
    "text_encoder = models['clip']\n",
    "decoder = models['decoder']\n",
    "unet = models['diffusion']\n",
    "ddpm = DDPMSampler(generator=None)\n",
    "\n",
    "# Disable gradient computations for the VAE, DDPM, and text_encoder models\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set the VAE and text_encoder to eval mode\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "# Separate parameters for discriminative tasks\n",
    "discriminative_params = []\n",
    "non_discriminative_params = []\n",
    "\n",
    "for name, param in unet.named_parameters():\n",
    "    if isinstance(getattr(unet, name.split('.')[0], None), (TransformerBlock, UNet_Transformer)):\n",
    "        discriminative_params.append(param)\n",
    "    else:\n",
    "        non_discriminative_params.append(param)\n",
    "\n",
    "# AdamW optimizer with separate learning rates\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': non_discriminative_params, 'lr': learning_rate},\n",
    "    {'params': discriminative_params, 'lr': discriminative_learning_rate}\n",
    "], betas=(adam_beta1, adam_beta2), weight_decay=adam_weight_decay, eps=adam_epsilon)\n",
    "\n",
    "# Linear warmup scheduler for non-discriminative parameters\n",
    "def warmup_lr_lambda(current_step: int):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[\n",
    "    warmup_lr_lambda,  # Apply warmup for non-discriminative params\n",
    "    lambda step: 1.0  # Keep constant learning rate for discriminative params\n",
    "])\n",
    "\n",
    "# EMA setup\n",
    "ema_unet = torch.optim.swa_utils.AveragedModel(unet, avg_fn=lambda averaged_model_parameter, model_parameter, num_averaged: ema_decay * averaged_model_parameter + (1 - ema_decay) * model_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_train_epochs, device=\"cuda\", save_steps=1000, max_train_steps=10000):\n",
    "    global_step = 0\n",
    "\n",
    "    best_loss = float('inf')  # Initialize best loss as infinity\n",
    "    best_step = 0\n",
    "    accumulator = 0\n",
    "\n",
    "    # Move models to the device\n",
    "    vae.to(device)\n",
    "    text_encoder.to(device)\n",
    "    unet.to(device)\n",
    "    ema_unet.to(device)\n",
    "\n",
    "    num_train_epochs = tqdm(range(first_epoch, num_train_epochs), desc=\"Epoch\")\n",
    "    for epoch in num_train_epochs:\n",
    "        train_loss = 0.0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Extract images and texts from batch\n",
    "            images = batch[\"pixel_values\"]\n",
    "            texts = batch[\"input_ids\"]\n",
    "\n",
    "            # Normalize pixel values to [-1, 1]\n",
    "            # images = images / 127.5 - 1.0\n",
    "\n",
    "            # Move batch to the device\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            # Encode images to latent space\n",
    "            encoder_noise = torch.randn(images.shape[0], 4, LATENTS_HEIGHT, LATENTS_WIDTH).to(device)  # Shape (BATCH_SIZE, 4, 32, 32)\n",
    "            latents = vae(images, encoder_noise)\n",
    "\n",
    "            # Sample noise and timesteps for diffusion process\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "            text_timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "\n",
    "            # Add noise to latents and texts\n",
    "            noisy_latents, image_noise = ddpm.add_noise(latents, timesteps)\n",
    "            encoder_hidden_states = text_encoder(texts)\n",
    "            noisy_text_query, text_noise = ddpm.add_noise(encoder_hidden_states, text_timesteps)\n",
    "\n",
    "            # Get time embeddings\n",
    "            image_time_embeddings = get_time_embedding(timesteps, is_image=True).to(device)\n",
    "            text_time_embeddings = get_time_embedding(timesteps, is_image=False).to(device)\n",
    "            \n",
    "            # Average and normalize text time embeddings\n",
    "            average_noisy_text_query = noisy_text_query.mean(dim=1)\n",
    "            text_query = F.normalize(average_noisy_text_query, p=2, dim=-1)\n",
    "\n",
    "            # Randomly drop 10% of text and image conditions: Context Free Guidance\n",
    "            if torch.rand(1).item() < 0.1:\n",
    "                text_query = torch.zeros_like(text_query)\n",
    "            if torch.rand(1).item() < 0.1:\n",
    "                noisy_latents = torch.zeros_like(noisy_latents)\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            image_pred, text_pred = unet(noisy_latents, encoder_hidden_states, image_time_embeddings, text_time_embeddings, text_query)\n",
    "            image_loss = F.mse_loss(image_pred.float(), image_noise.float(), reduction=\"mean\")\n",
    "            text_loss = F.mse_loss(text_pred.float(), text_query.float(), reduction=\"mean\")\n",
    "            \n",
    "            loss = image_loss + Lambda * text_loss\n",
    "            train_loss += loss.item()\n",
    "            accumulator += loss.item()\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            ema_unet.update_parameters(unet)\n",
    "\n",
    "            if global_step % save_steps == 0 and global_step != 0:\n",
    "                # Save model and optimizer state\n",
    "                save_path = os.path.join(output_dir, f\"last.pt\")\n",
    "                torch.save({\n",
    "                    'model_state_dict': unet.state_dict(),\n",
    "                    'ema_state_dict': ema_unet.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, save_path)\n",
    "                print(f\"\\nSaved state to {save_path}\")\n",
    "\n",
    "                # Check if the current step's loss is the best\n",
    "                if accumulator / save_steps < best_loss:\n",
    "                    best_loss = accumulator / save_steps\n",
    "                    best_step = global_step\n",
    "                    best_save_path = os.path.join(output_dir, \"best.pt\")\n",
    "                    torch.save({\n",
    "                        'model_state_dict': unet.state_dict(),\n",
    "                        'ema_state_dict': ema_unet.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }, best_save_path)\n",
    "                    print(f\"New best model saved to {best_save_path} with loss {best_loss}\")\n",
    "\n",
    "                s = 'Epoch: %d   Step: %d   Loss: %.5f   Best Loss: %.5f   Best Step: %d\\n' % (epoch, global_step, accumulator / save_steps, best_loss, best_step)\n",
    "                print(s)\n",
    "                with open(os.path.join(output_dir, 'train_log.txt'), 'a') as f:\n",
    "                    f.write(s)\n",
    "\n",
    "                accumulator = 0.0\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"Step: {global_step}  Loss: {loss.item()}  Time: {end_time - start_time}\")\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        print(f\"Average loss over epoch: {train_loss / (step + 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '==> Training starts..'\n",
    "s += f'\\nModel file: {model_file}'\n",
    "s += f'\\nBatch size: {BATCH_SIZE}'\n",
    "s += f'\\nWidth: {WIDTH}'\n",
    "s += f'\\nHeight: {HEIGHT}'\n",
    "s += f'\\nLatents width: {LATENTS_WIDTH}'\n",
    "s += f'\\nLatents height: {LATENTS_HEIGHT}'\n",
    "s += f'\\nFirst epoch: {first_epoch}'\n",
    "s += f'\\nNumber of training epochs: {num_train_epochs}'\n",
    "s += f'\\nLambda: {Lambda}'\n",
    "s += f'\\nLearning rate: {learning_rate}'\n",
    "s += f'\\nDiscriminative learning rate: {discriminative_learning_rate}'\n",
    "s += f'\\nAdam beta1: {adam_beta1}'\n",
    "s += f'\\nAdam beta2: {adam_beta2}'\n",
    "s += f'\\nAdam weight decay: {adam_weight_decay}'\n",
    "s += f'\\nAdam epsilon: {adam_epsilon}'\n",
    "s += f'\\nEMA decay: {ema_decay}'\n",
    "s += f'\\nWarmup steps: {warmup_steps}'\n",
    "s += f'\\nOutput directory: {output_dir}'\n",
    "s += f'\\nSave steps: {save_steps}'\n",
    "s += f'\\nMax train steps: {max_train_steps}'\n",
    "s += f'\\nDevice: {DEVICE}'\n",
    "s += f'\\n\\n'\n",
    "print(s)\n",
    "\n",
    "# Create the output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'train_log.txt'), 'w') as f:\n",
    "    f.write(s)\n",
    "\n",
    "train(num_train_epochs=num_train_epochs, device=DEVICE, save_steps=save_steps, max_train_steps=max_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_loader\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "tokenizer = CLIPTokenizer(\"./data/vocab.json\", merges_file=\"./data/merges.txt\")\n",
    "model_file = \"./data/v1-5-pruned.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "## TEXT TO IMAGE\n",
    "\n",
    "# prompt = \"A dog with sunglasses, wearing comfy hat, looking at camera, highly detailed, ultra sharp, cinematic, 100mm lens, 8k resolution.\"\n",
    "prompt = \"Two students suffering from completing the generative model course project, highly detailed, ultra sharp, cinematic, 100mm lens, 8k resolution.\"\n",
    "uncond_prompt = \"\"  # Also known as negative prompt\n",
    "do_cfg = False\n",
    "cfg_scale = 8  # min: 1, max: 14\n",
    "\n",
    "## SAMPLER\n",
    "\n",
    "sampler = \"ddpm\"\n",
    "num_inference_steps = 50\n",
    "seed = 42\n",
    "\n",
    "output_image = pipeline.generate(\n",
    "    prompt=prompt,\n",
    "    uncond_prompt=uncond_prompt,\n",
    "    input_image=None,\n",
    "    strength=0.9,\n",
    "    do_cfg=do_cfg,\n",
    "    cfg_scale=cfg_scale,\n",
    "    sampler_name=sampler,\n",
    "    n_inference_steps=num_inference_steps,\n",
    "    seed=seed,\n",
    "    models=models,\n",
    "    device=DEVICE,\n",
    "    idle_device=\"cpu\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Combine the input image and the output image into a single image.\n",
    "Image.fromarray(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT REMARK** For the purpose of validating the model's training process, we have opted to utilize a representative dummy dataset. This dataset, while not the actual one, mirrors the format of the original CC3M dataset. It's worth noting that the CC3M dataset is considerably large, with a total size of approximately 430 GB. Given this substantial size, and considering our resource constraints, we have determined that it would not be feasible to complete the training process prior to the deadline for the first version, which is set for May 5. However, we have devised a plan to ensure that the training process is completed by the subsequent deadline for the second version, scheduled for May 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the implementation of the model as described in the research paper, we encountered several areas of ambiguity that necessitated the formulation of certain assumptions. These assumptions, which guided our implementation, are detailed as follows:\n",
    "\n",
    "1) The paper did not provide explicit information regarding whether the transformers employed for the image-to-text alignment task were part of a separate architecture or integrated within the proposed UNet middle blocks. Given this lack of clarity, our implementation treats the transformers as a distinct architecture.\n",
    "\n",
    "2) The paper did not specify which blocks were to be modified to incorporate the dual stream deep fusion blocks. In our implementation, we have chosen to integrate these blocks into both the downsample and middle blocks of the original UNet, also known as Stable Diffusion. Notably, we did not apply these changes to the upsample layers, as they are not utilized in the image-to-text alignment task.\n",
    "\n",
    "3) The term 'text query' was used in the paper without a clear definition. In our interpretation, we have chosen to represent the text query as the normalized average of the output from the text encoder.\n",
    "\n",
    "4) The paper did not provide a clear methodology for the concatenation of the hidden latent image and the output of the fully connected layer. In our implementation, we expanded the output of the fully connected layer from a shape of (Batch Size, Channels) to (Batch Size, Channels, Height, Width), enabling its concatenation with the latent image, which also has a shape of (Batch Size, Channels, Height, Width).\n",
    "\n",
    "5) The paper did not provide explicit instructions on how the fully connected layer projects the text query back into the text embedding space. We assumed that it generates an output with a shape of (Batch Size, Width * Height, 768). We then computed the normalized average of this output along dimension 1, resulting in an output of shape (Batch Size, 768). This output serves as the hidden text query that is input to the next layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
