{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paper Name:** DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability\n",
    "\n",
    "**Link:** https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_DiffDis_Empowering_Generative_Diffusion_Model_with_Cross-Modal_Discrimination_Capability_ICCV_2023_paper.pd\n",
    "\n",
    "**Project Members:**\n",
    "Furkan Genç,\n",
    "Barış Sarper Tezcan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the constants \n",
    "WIDTH = 512\n",
    "HEIGHT = 512\n",
    "LATENTS_WIDTH = WIDTH // 8\n",
    "LATENTS_HEIGHT = HEIGHT // 8\n",
    "BATCH_SIZE = 1\n",
    "root_dir = \"../dataset/cc3m/train\" # TODO: change root_dir with the path to the dataset according to your setup\n",
    "\n",
    "# training parameters\n",
    "num_train_epochs = 6\n",
    "Lambda = 1.0\n",
    "save_steps = 1000\n",
    "\n",
    "# optimizer parameters\n",
    "learning_rate = 1e-5\n",
    "discriminative_learning_rate = 1e-4  # New learning rate for discriminative tasks\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_weight_decay = 1e-4\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# IMAGE TO TEXT\n",
    "test_dataset = \"CIFAR10\"  # Set to \"CIFAR100\" to use CIFAR100 dataset\n",
    "\n",
    "# output directory\n",
    "train_output_dir = \"../results/output_1\"\n",
    "test_output_dir = \"../results/\" + test_dataset\n",
    "inference_output_dir = \"../results/text_to_image/output_1/last\"\n",
    "\n",
    "# Load the models\n",
    "model_file = \"data/v1-5-pruned.ckpt\"  \n",
    "train_unet_file = None  # Set to None to finetune from scratch, if specified, the diffusion model will be loaded from this file\n",
    "test_unet_file = \"../results/output_1/last.pt\" \n",
    "inference_unet_file = \"../results/output_1/last.pt\"\n",
    "\n",
    "# EMA parameters\n",
    "use_ema = False  # Set to True to use EMA\n",
    "ema_decay = 0.9999\n",
    "warmup_steps = 1000\n",
    "\n",
    "# TEXT TO IMAGE\n",
    "prompt1 = \"A river with boats docked and houses in the background\"\n",
    "prompt2 = \"A piece of chocolate swirled cake on a plate\"\n",
    "prompt3 = \"A large bed sitting next to a small Christmas Tree surrounded by pictures\"\n",
    "prompt4 = \"A bear searching for food near the river\"\n",
    "prompts = [prompt1, prompt2, prompt3, prompt4]\n",
    "uncond_prompt = \"\"  # Also known as negative prompt\n",
    "do_cfg = True\n",
    "cfg_scale = 3  # min: 1, max: 14\n",
    "num_samples = 1\n",
    "\n",
    "# SAMPLER\n",
    "sampler = \"ddpm\"\n",
    "num_inference_steps = 50\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from ddpm import DDPMSampler\n",
    "from pipeline import get_time_embedding\n",
    "from dataloader import train_dataloader\n",
    "import model_loader\n",
    "import time\n",
    "from diffusion import TransformerBlock, UNet_Transformer  # Ensure these are correctly imported\n",
    "\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# Set the device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the models\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "ddpm = DDPMSampler(generator=None)\n",
    "\n",
    "if train_unet_file is not None:\n",
    "    # Load the UNet model\n",
    "    print(f\"Loading UNet model from {train_unet_file}\")\n",
    "    models['diffusion'].load_state_dict(torch.load(train_unet_file)['model_state_dict'])\n",
    "    if 'best_loss' in torch.load(train_unet_file):\n",
    "        best_loss = torch.load(train_unet_file)['best_loss']\n",
    "        best_step = torch.load(train_unet_file)['best_step']\n",
    "        last_loss = torch.load(train_unet_file)['last_loss']\n",
    "        last_step = torch.load(train_unet_file)['last_step']\n",
    "    else:\n",
    "        best_loss = float('inf')\n",
    "        best_step = 0\n",
    "        last_loss = 0.0\n",
    "        last_step = 0\n",
    "else:\n",
    "    best_loss = float('inf')\n",
    "    best_step = 0\n",
    "    last_loss = 0.0\n",
    "    last_step = 0\n",
    "\n",
    "# TEXT TO IMAGE\n",
    "tokenizer = CLIPTokenizer(\"./data/vocab.json\", merges_file=\"./data/merges.txt\")\n",
    "\n",
    "# Disable gradient computations for the models['encoder'], DDPM, and models['clip'] models\n",
    "for param in models['encoder'].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in models['clip'].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set the models['encoder'] and models['clip'] to eval mode\n",
    "models['encoder'].eval()\n",
    "models['clip'].eval()\n",
    "\n",
    "# Separate parameters for discriminative tasks\n",
    "discriminative_params = []\n",
    "non_discriminative_params = []\n",
    "\n",
    "for name, param in models['diffusion'].named_parameters():\n",
    "    if isinstance(getattr(models['diffusion'], name.split('.')[0], None), (TransformerBlock, UNet_Transformer)):\n",
    "        discriminative_params.append(param)\n",
    "    else:\n",
    "        non_discriminative_params.append(param)\n",
    "\n",
    "# AdamW optimizer with separate learning rates\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': non_discriminative_params, 'lr': learning_rate},\n",
    "    {'params': discriminative_params, 'lr': discriminative_learning_rate}\n",
    "], betas=(adam_beta1, adam_beta2), weight_decay=adam_weight_decay, eps=adam_epsilon)\n",
    "\n",
    "if train_unet_file is not None:\n",
    "    print(f\"Loading optimizer state from {train_unet_file}\")\n",
    "    optimizer.load_state_dict(torch.load(train_unet_file)['optimizer_state_dict'])\n",
    "\n",
    "# Linear warmup scheduler for non-discriminative parameters\n",
    "def warmup_lr_lambda(current_step: int):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[\n",
    "    warmup_lr_lambda,  # Apply warmup for non-discriminative params\n",
    "    lambda step: 1.0  # Keep constant learning rate for discriminative params\n",
    "])\n",
    "\n",
    "# EMA setup\n",
    "if use_ema:\n",
    "    ema_unet = torch.optim.swa_utils.AveragedModel(models['diffusion'], avg_fn=lambda averaged_model_parameter, model_parameter, num_averaged: ema_decay * averaged_model_parameter + (1 - ema_decay) * model_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_train_epochs, device=\"cuda\", save_steps=1000):\n",
    "    global best_loss, best_step, last_loss, last_step\n",
    "\n",
    "    if train_unet_file is not None:\n",
    "        first_epoch = last_step // len(train_dataloader)\n",
    "        global_step = last_step + 1\n",
    "    else:\n",
    "        first_epoch = 0\n",
    "        global_step = 0\n",
    "\n",
    "    accumulator = 0\n",
    "\n",
    "    # Move models to the device\n",
    "    models['encoder'].to(device)\n",
    "    models['clip'].to(device)\n",
    "    models['diffusion'].to(device)\n",
    "    if use_ema:\n",
    "        ema_unet.to(device)\n",
    "\n",
    "    num_train_epochs = tqdm(range(first_epoch, num_train_epochs), desc=\"Epoch\")\n",
    "    for epoch in num_train_epochs:\n",
    "        train_loss = 0.0\n",
    "        num_train_steps = len(train_dataloader)\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Extract images and texts from batch\n",
    "            images = batch[\"pixel_values\"]\n",
    "            texts = batch[\"input_ids\"]\n",
    "\n",
    "            # Move batch to the device\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            # Encode images to latent space\n",
    "            encoder_noise = torch.randn(images.shape[0], 4, LATENTS_HEIGHT, LATENTS_WIDTH).to(device)  # Shape (BATCH_SIZE, 4, 32, 32)\n",
    "            latents = models['encoder'](images, encoder_noise)\n",
    "\n",
    "            # Sample noise and timesteps for diffusion process\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "            text_timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "\n",
    "            # Add noise to latents and texts\n",
    "            noisy_latents, image_noise = ddpm.add_noise(latents, timesteps)\n",
    "            encoder_hidden_states = models['clip'](texts)\n",
    "            noisy_text_query, text_noise = ddpm.add_noise(encoder_hidden_states, text_timesteps)\n",
    "\n",
    "            # Get time embeddings\n",
    "            image_time_embeddings = get_time_embedding(timesteps, is_image=True).to(device)\n",
    "            text_time_embeddings = get_time_embedding(timesteps, is_image=False).to(device)\n",
    "            \n",
    "            # Average and normalize text time embeddings\n",
    "            average_noisy_text_query = noisy_text_query.mean(dim=1)\n",
    "            text_query = F.normalize(average_noisy_text_query, p=2, dim=-1)\n",
    "\n",
    "            # Randomly drop 10% of text and image conditions: Context Free Guidance\n",
    "            if torch.rand(1).item() < 0.1:\n",
    "                text_query = torch.zeros_like(text_query)\n",
    "            if torch.rand(1).item() < 0.1:\n",
    "                noisy_latents = torch.zeros_like(noisy_latents)\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            image_pred, text_pred = models['diffusion'](noisy_latents, encoder_hidden_states, image_time_embeddings, text_time_embeddings, text_query)\n",
    "            image_loss = F.mse_loss(image_pred.float(), image_noise.float(), reduction=\"mean\")\n",
    "            text_loss = F.mse_loss(text_pred.float(), text_query.float(), reduction=\"mean\")\n",
    "\n",
    "            loss = image_loss + Lambda * text_loss\n",
    "            train_loss += loss.item()\n",
    "            accumulator += loss.item()\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            if use_ema:\n",
    "                ema_unet.update_parameters(models['diffusion'])\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            if train_unet_file is not None and epoch == first_epoch:\n",
    "                print(f\"Step: {step+1+last_step}/{num_train_steps+last_step}   Loss: {loss.item()}   Time: {end_time - start_time}\", end=\"\\r\")\n",
    "            else:\n",
    "                print(f\"Step: {step}/{num_train_steps}   Loss: {loss.item()}   Time: {end_time - start_time}\", end=\"\\r\")\n",
    "\n",
    "            if global_step % save_steps == 0 and global_step > 0:\n",
    "                # Check if the current step's loss is the best\n",
    "                if accumulator / save_steps < best_loss:\n",
    "                    best_loss = accumulator / save_steps\n",
    "                    best_step = global_step\n",
    "                    best_save_path = os.path.join(train_output_dir, \"best.pt\")\n",
    "                    if use_ema:\n",
    "                        torch.save({\n",
    "                            'model_state_dict': models['diffusion'].state_dict(),\n",
    "                            'ema_state_dict': ema_unet.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_loss': best_loss,\n",
    "                            'best_step': best_step,\n",
    "                            'last_loss': accumulator / save_steps,\n",
    "                            'last_step': global_step\n",
    "                        }, best_save_path) \n",
    "                    else:\n",
    "                        torch.save({\n",
    "                            'model_state_dict': models['diffusion'].state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_loss': best_loss,\n",
    "                            'best_step': best_step,\n",
    "                            'last_loss': accumulator / save_steps,\n",
    "                            'last_step': global_step\n",
    "                        }, best_save_path)              \n",
    "\n",
    "                    print(f\"\\nNew best model saved to {best_save_path} with loss {best_loss}\")\n",
    "\n",
    "                # Save model and optimizer state\n",
    "                last_save_path = os.path.join(train_output_dir, f\"last.pt\")\n",
    "                if use_ema:\n",
    "                    torch.save({\n",
    "                        'model_state_dict': models['diffusion'].state_dict(),\n",
    "                        'ema_state_dict': ema_unet.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'best_loss': best_loss,\n",
    "                        'best_step': best_step,\n",
    "                        'last_loss': accumulator / save_steps,\n",
    "                        'last_step': global_step\n",
    "                    }, last_save_path)\n",
    "                else:\n",
    "                    torch.save({\n",
    "                        'model_state_dict': models['diffusion'].state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'best_loss': best_loss,\n",
    "                        'best_step': best_step,\n",
    "                        'last_loss': accumulator / save_steps,\n",
    "                        'last_step': global_step\n",
    "                    }, last_save_path)\n",
    "                    \n",
    "                print(f\"Saved state to {last_save_path}\")\n",
    "\n",
    "                # Generate samples from the model\n",
    "                for i, prompt in enumerate(prompts):\n",
    "                    # Sample images from the model\n",
    "                    output_image = pipeline.generate(\n",
    "                        prompt=prompt,\n",
    "                        uncond_prompt=uncond_prompt,\n",
    "                        input_image=None,\n",
    "                        strength=0.9,\n",
    "                        do_cfg=do_cfg,\n",
    "                        cfg_scale=cfg_scale,\n",
    "                        sampler_name=sampler,\n",
    "                        n_inference_steps=num_inference_steps,\n",
    "                        seed=seed,\n",
    "                        models=models,\n",
    "                        device=DEVICE,\n",
    "                        idle_device=DEVICE,\n",
    "                        tokenizer=tokenizer,\n",
    "                    )\n",
    "\n",
    "                    # Save the generated image\n",
    "                    output_image = Image.fromarray(output_image)\n",
    "                    output_image.save(os.path.join(train_output_dir, \"images\", \"prompt\" + str(i+1), f\"step{global_step}.png\"))\n",
    "                \n",
    "                print(f\"\\nSaved images for step {global_step}\")\n",
    "                print('Epoch: %d   Step: %d   Loss: %.5f   Best Loss: %.5f   Best Step: %d\\n' % (epoch+1, global_step, accumulator / save_steps, best_loss, best_step))\n",
    "\n",
    "                accumulator = 0.0\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        print(f\"Average loss over epoch: {train_loss / (step + 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '==> Training starts..'\n",
    "s += f'\\n\\nModel file: {model_file}'\n",
    "s += f'\\nUNet file: {train_unet_file}'\n",
    "s += f'\\nBatch size: {BATCH_SIZE}'\n",
    "s += f'\\nWidth: {WIDTH}'\n",
    "s += f'\\nHeight: {HEIGHT}'\n",
    "s += f'\\nLatents width: {LATENTS_WIDTH}'\n",
    "s += f'\\nLatents height: {LATENTS_HEIGHT}'\n",
    "s += f'\\nFirst epoch: {last_step // len(train_dataloader)}'\n",
    "s += f'\\nNumber of training epochs: {num_train_epochs}'\n",
    "s += f'\\nLambda: {Lambda}'\n",
    "s += f'\\nLearning rate: {learning_rate}'\n",
    "s += f'\\nDiscriminative learning rate: {discriminative_learning_rate}'\n",
    "s += f'\\nAdam beta1: {adam_beta1}'\n",
    "s += f'\\nAdam beta2: {adam_beta2}'\n",
    "s += f'\\nAdam weight decay: {adam_weight_decay}'\n",
    "s += f'\\nAdam epsilon: {adam_epsilon}'\n",
    "s += f'\\nUse EMA: {use_ema}'\n",
    "s += f'\\nEMA decay: {ema_decay}'\n",
    "s += f'\\nWarmup steps: {warmup_steps}'\n",
    "s += f'\\nOutput directory: {train_output_dir}'\n",
    "s += f'\\nSave steps: {save_steps}'\n",
    "s += f'\\nDevice: {DEVICE}'\n",
    "s += f'\\nSampler: {sampler}'\n",
    "s += f'\\nNumber of inference steps: {num_inference_steps}'\n",
    "s += f'\\nSeed: {seed}'\n",
    "for i, prompt in enumerate(prompts):\n",
    "    s += f'\\nPrompt {i + 1}: {prompt}'\n",
    "s += f'\\nUnconditional prompt: {uncond_prompt}'\n",
    "s += f'\\nDo CFG: {do_cfg}'\n",
    "s += f'\\nCFG scale: {cfg_scale}'\n",
    "s += f'\\n\\n'\n",
    "print(s)\n",
    "\n",
    "\n",
    "train(num_train_epochs=num_train_epochs, device=DEVICE, save_steps=save_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from ddpm import DDPMSampler\n",
    "from pipeline import get_time_embedding\n",
    "import model_loader\n",
    "import time\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# Set the device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the models\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "ddpm = DDPMSampler(generator=None)\n",
    "\n",
    "if test_unet_file is not None:\n",
    "    # Load the UNet model\n",
    "    print(f\"Loading UNet model from {test_unet_file}\")\n",
    "    if use_ema:\n",
    "        models['diffusion'].load_state_dict(torch.load(test_unet_file)['ema_state_dict'])\n",
    "    else:\n",
    "        models['diffusion'].load_state_dict(torch.load(test_unet_file)['model_state_dict'])\n",
    "\n",
    "# TEXT TO IMAGE\n",
    "tokenizer = CLIPTokenizer(\"./data/vocab.json\", merges_file=\"./data/merges.txt\")\n",
    "\n",
    "# Set the models['encoder'], models['clip'], models['diffusion'] to eval mode\n",
    "models['encoder'].eval()\n",
    "models['clip'].eval()\n",
    "models['diffusion'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device=\"cuda\"):\n",
    "    # Get the transform for the test data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((WIDTH, HEIGHT), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    # Load the CIFAR-10 dataset\n",
    "    if test_dataset == \"CIFAR10\":\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root='../dataset', train=False, download=True, transform=transform)\n",
    "\n",
    "    elif test_dataset == \"CIFAR100\":\n",
    "        testset = torchvision.datasets.CIFAR100(\n",
    "            root='../dataset', train=False, download=True, transform=transform)\n",
    "\n",
    "    print(f\"Test dataset: {test_dataset} | Number of test samples: {len(testset)}\")\n",
    "\n",
    "    # Load the test data\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Move models to the device\n",
    "    models['encoder'].to(device)\n",
    "    models['clip'].to(device)\n",
    "    models['diffusion'].to(device)\n",
    "\n",
    "    # Define the class names and tokens\n",
    "    class_names = testset.classes\n",
    "    class_tokens = []\n",
    "\n",
    "    # Tokenize class names\n",
    "    for class_name in class_names:\n",
    "        # Tokenize text\n",
    "        tokens = tokenizer.batch_encode_plus(\n",
    "            [class_name], padding=\"max_length\", max_length=77\n",
    "        ).input_ids\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long).squeeze()\n",
    "        class_tokens.append(tokens)\n",
    "\n",
    "    # Convert list of class tokens to a tensor\n",
    "    class_tokens = torch.stack(class_tokens).to(device)\n",
    "    print(f\"Class tokens shape: {class_tokens.shape}\")\n",
    "\n",
    "    # Encode class tokens with the CLIP model\n",
    "    with torch.no_grad():\n",
    "        # Encode class tokens\n",
    "        encoder_hidden_states = models['clip'](class_tokens)\n",
    "\n",
    "        # Average and normalize class embeddings\n",
    "        class_embeddings = encoder_hidden_states.mean(dim=1)\n",
    "        class_embeddings = F.normalize(class_embeddings, p=2, dim=-1)\n",
    "        print(f\"Class embeddings shape: {class_embeddings.shape}\\n\")\n",
    "    \n",
    "    # Start testing\n",
    "    test_loss = 0.0\n",
    "    num_test_steps = len(testloader)\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(testloader):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Move batch to the device\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            texts = [class_tokens[target] for target in targets]\n",
    "            \n",
    "            # Convert list of class tokens to a tensor\n",
    "            texts = torch.stack(texts).to(device)\n",
    "\n",
    "            # Encode images to latent space\n",
    "            encoder_noise = torch.randn(images.shape[0], 4, LATENTS_HEIGHT, LATENTS_WIDTH).to(device)  # Shape (BATCH_SIZE, 4, 32, 32)\n",
    "            latents = models['encoder'](images, encoder_noise)\n",
    "\n",
    "            # Sample noise and timesteps for diffusion process\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "            text_timesteps = torch.randint(0, ddpm.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "\n",
    "            # Add noise to latents and texts\n",
    "            noisy_latents, image_noise = ddpm.add_noise(latents, timesteps)\n",
    "            encoder_hidden_states = models['clip'](texts)\n",
    "            noisy_text_query, text_noise = ddpm.add_noise(encoder_hidden_states, text_timesteps)\n",
    "\n",
    "            # Get time embeddings\n",
    "            image_time_embeddings = get_time_embedding(timesteps, is_image=True).to(device)\n",
    "            text_time_embeddings = get_time_embedding(timesteps, is_image=False).to(device)\n",
    "            \n",
    "            # Average and normalize text time embeddings\n",
    "            average_noisy_text_query = noisy_text_query.mean(dim=1)\n",
    "            text_query = F.normalize(average_noisy_text_query, p=2, dim=-1)\n",
    "\n",
    "            # Randomly drop 10% of text and image conditions: Context Free Guidance\n",
    "            if torch.rand(1).item() < 0.1:\n",
    "                text_query = torch.zeros_like(text_query)\n",
    "            if torch.rand(1).item() < 0.1:\n",
    "                noisy_latents = torch.zeros_like(noisy_latents)\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            _, text_pred = models['diffusion'](noisy_latents, encoder_hidden_states, image_time_embeddings, text_time_embeddings, text_query)\n",
    "                \n",
    "            # Calculate loss\n",
    "            loss = F.mse_loss(text_pred.float(), text_query.float(), reduction=\"mean\")\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate cosine similarity between the generated text query and class embeddings\n",
    "            similarities = F.cosine_similarity(text_pred.unsqueeze(1), class_embeddings.unsqueeze(0), dim=-1)\n",
    "            predicted_classes = similarities.argmax(dim=-1)\n",
    "\n",
    "            # Compare predictions with actual targets\n",
    "            correct_predictions += (predicted_classes == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(f\"Batch {batch_idx + 1}/{num_test_steps} | Loss: {loss:.4f} | Time: {end_time - start_time:.2f}s\", end=\"\\r\")\n",
    "\n",
    "    # Calculate total accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    s = f\"Accuracy: {correct_predictions}/{total_predictions} ({accuracy:.4f})\\n\"\n",
    "    s += f\"\\nTest Loss: {test_loss / num_test_steps:.4f}\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '==> Testing starts..'\n",
    "s += f'\\n\\nTest dataset: {test_dataset}'\n",
    "s += f'\\nModel file: {model_file}'\n",
    "s += f'\\nUNet file: {test_unet_file}'\n",
    "s += f'\\nBatch size: {BATCH_SIZE}'\n",
    "s += f'\\nWidth: {WIDTH}'\n",
    "s += f'\\nHeight: {HEIGHT}'\n",
    "s += f'\\nLatents width: {LATENTS_WIDTH}'\n",
    "s += f'\\nLatents height: {LATENTS_HEIGHT}'\n",
    "s += f'\\nNumber of training epochs: {num_train_epochs}'\n",
    "s += f'\\nLambda: {Lambda}'\n",
    "s += f'\\nLearning rate: {learning_rate}'\n",
    "s += f'\\nDiscriminative learning rate: {discriminative_learning_rate}'\n",
    "s += f'\\nAdam beta1: {adam_beta1}'\n",
    "s += f'\\nAdam beta2: {adam_beta2}'\n",
    "s += f'\\nAdam weight decay: {adam_weight_decay}'\n",
    "s += f'\\nAdam epsilon: {adam_epsilon}'\n",
    "s += f'\\nUse EMA: {use_ema}'\n",
    "s += f'\\nEMA decay: {ema_decay}'\n",
    "s += f'\\nWarmup steps: {warmup_steps}'\n",
    "s += f'\\nOutput directory: {test_output_dir}'\n",
    "s += f'\\nSave steps: {save_steps}'\n",
    "s += f'\\nDevice: {DEVICE}'\n",
    "s += f'\\nSampler: {sampler}'\n",
    "s += f'\\nNumber of inference steps: {num_inference_steps}'\n",
    "s += f'\\nSeed: {seed}'\n",
    "for i, prompt in enumerate(prompts):\n",
    "    s += f'\\nPrompt {i + 1}: {prompt}'\n",
    "s += f'\\nUnconditional prompt: {uncond_prompt}'\n",
    "s += f'\\nDo CFG: {do_cfg}'\n",
    "s += f'\\nCFG scale: {cfg_scale}'\n",
    "s += f'\\n\\n'\n",
    "print(s)\n",
    "\n",
    "test(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import model_loader\n",
    "import time\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer\n",
    "from IPython.display import display\n",
    "\n",
    "# Set the device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the models\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "if inference_unet_file is not None:\n",
    "    # Load the UNet model\n",
    "    print(f\"Loading UNet model from {inference_unet_file}\")\n",
    "    models['diffusion'].load_state_dict(torch.load(inference_unet_file)['model_state_dict'])\n",
    "\n",
    "# TEXT TO IMAGE\n",
    "tokenizer = CLIPTokenizer(\"./data/vocab.json\", merges_file=\"./data/merges.txt\")\n",
    "\n",
    "# Generate samples from the model\n",
    "for i, prompt in enumerate(prompts):\n",
    "    for j in range(num_samples):\n",
    "        start = time.time()\n",
    "\n",
    "        # Sample images from the model\n",
    "        output_image = pipeline.generate(\n",
    "            prompt=prompt,\n",
    "            uncond_prompt=uncond_prompt,\n",
    "            input_image=None,\n",
    "            strength=0.9,\n",
    "            do_cfg=do_cfg,\n",
    "            cfg_scale=cfg_scale,\n",
    "            sampler_name=sampler,\n",
    "            n_inference_steps=num_inference_steps,\n",
    "            seed=seed,\n",
    "            models=models,\n",
    "            device=DEVICE,\n",
    "            idle_device=DEVICE,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "        print(f\"PROMPT {i+1} - SAMPLE {j+1} - TIME: {end - start:.2f}s\\n\")\n",
    "\n",
    "        # Save the generated image\n",
    "        output_image = Image.fromarray(output_image)\n",
    "        \n",
    "        # Display the generated image\n",
    "        display(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT REMARK** For the purpose of validating the model's training process, we have opted to utilize a representative dummy dataset. This dataset, while not the actual one, mirrors the format of the original CC3M dataset. It's worth noting that the CC3M dataset is considerably large, with a total size of approximately 430 GB. Given this substantial size, and considering our resource constraints, we have determined that it would not be feasible to complete the training process prior to the deadline for the first version, which is set for May 5. However, we have devised a plan to ensure that the training process is completed by the subsequent deadline for the second version, scheduled for May 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the implementation of the model as described in the research paper, we encountered several areas of ambiguity that necessitated the formulation of certain assumptions. These assumptions, which guided our implementation, are detailed as follows:\n",
    "\n",
    "1) The paper did not provide explicit information regarding whether the transformers employed for the image-to-text alignment task were part of a separate architecture or integrated within the proposed UNet middle blocks. Given this lack of clarity, our implementation treats the transformers as a distinct architecture.\n",
    "\n",
    "2) The paper did not specify which blocks were to be modified to incorporate the dual stream deep fusion blocks. In our implementation, we have chosen to integrate these blocks into both the downsample and middle blocks of the original UNet, also known as Stable Diffusion. Notably, we did not apply these changes to the upsample layers, as they are not utilized in the image-to-text alignment task.\n",
    "\n",
    "3) The term 'text query' was used in the paper without a clear definition. In our interpretation, we have chosen to represent the text query as the normalized average of the output from the text encoder.\n",
    "\n",
    "4) The paper did not provide a clear methodology for the concatenation of the hidden latent image and the output of the fully connected layer. In our implementation, we expanded the output of the fully connected layer from a shape of (Batch Size, Channels) to (Batch Size, Channels, Height, Width), enabling its concatenation with the latent image, which also has a shape of (Batch Size, Channels, Height, Width).\n",
    "\n",
    "5) The paper did not provide explicit instructions on how the fully connected layer projects the text query back into the text embedding space. We assumed that it generates an output with a shape of (Batch Size, Width * Height, 768). We then computed the normalized average of this output along dimension 1, resulting in an output of shape (Batch Size, 768). This output serves as the hidden text query that is input to the next layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
